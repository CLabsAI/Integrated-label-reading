import cv2
import base64
import tkinter as tk
from tkinter import messagebox, simpledialog, Scale
from googleapiclient.discovery import build
import os
import pickle
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from datetime import datetime
import numpy as np
from PIL import Image, ImageTk
import time

# Your Vision API key
vision_api_key = 'AIzaSyDRB4CbKeF9ORqFpjHuPAHyY-S-O-UpVxc'

# Google Sheets API
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
SPREADSHEET_ID = '1qMGdUnf9elwG6JAVa8Zojcype6UpaUCdk64sDESnIk0'
RANGE_NAME = 'IN'

# Build the Vision API client
vision_service = build('vision', 'v1', developerKey=vision_api_key)

# Function to increase brightness
def increase_brightness(img, value=30):
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h, s, v = cv2.split(hsv)

    lim = 255 - value
    v[v > lim] = 255
    v[v <= lim] += value

    final_hsv = cv2.merge((h, s, v))
    bright_img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)
    return bright_img

# Image preprocessing function
def preprocess_image(image, level=1):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    if level == 1:
        return gray
    elif level == 2:
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        return blurred
    elif level == 3:
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)
        return thresh
    elif level == 4:
        blurred = cv2.GaussianBlur(gray, (7, 7), 0)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        enhanced_contrast = clahe.apply(blurred)
        binary_image = cv2.adaptiveThreshold(enhanced_contrast, 255, 
                                             cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                             cv2.THRESH_BINARY, 15, 4)
        return binary_image

# Function to send the preprocessed image to Vision API and get annotations
def send_to_vision(image):
    _, buffer = cv2.imencode('.png', image)
    encoded_image = base64.b64encode(buffer).decode('UTF-8')
    request_body = {
        'requests': [{
            'image': {
                'content': encoded_image
            },
            'features': [{
                'type': 'TEXT_DETECTION'
            }]
        }]
    }

    response = vision_service.images().annotate(body=request_body).execute()
    if 'textAnnotations' not in response['responses'][0]:
        print("No text annotations found in the Vision API response.")
        return None, None  # Return None for both text and annotations

    raw_text = response['responses'][0]['textAnnotations'][0]['description']
    return raw_text, response  # Return the text and the entire response (which contains annotations)

# Function to send extracted data to Google Sheets
def send_to_google_sheets(data):
    creds = None
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)
        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)

    service = build('sheets', 'v4', credentials=creds)

    values = [[str(datetime.now()), data]]
    body = {
        'values': values
    }
    result = service.spreadsheets().values().append(
        spreadsheetId=SPREADSHEET_ID, range=RANGE_NAME,
        valueInputOption="RAW", body=body).execute()

# Global variable to store the captured frame
captured_frame = None

def capture_image_from_camera(brightness_factor=1.0):
    global captured_frame  # Make the variable global
    camera_index = 0
    cap = cv2.VideoCapture(camera_index)
    if not cap.isOpened():
        print(f"Error: Could not open the camera with index {camera_index}")
        return

    time.sleep(camera_duration.get())  # Using the camera_duration value
    ret, frame = cap.read()
    frame = np.clip(frame * brightness_factor, 0, 255).astype(np.uint8)
    captured_frame = frame  # Store the frame in the global variable
    cap.release()
    cv2.destroyAllWindows()

    # Display the captured image immediately
    display_single_image(captured_frame)

def display_image(original_image, preprocessed_image, annotations=None):
    window = tk.Toplevel(root)
    
    # Convert grayscale back to 3-channel image
    preprocessed_image = cv2.cvtColor(preprocessed_image, cv2.COLOR_GRAY2BGR)
    
    # Resize preprocessed image to match the original image dimensions
    preprocessed_image = cv2.resize(preprocessed_image, (original_image.shape[1], original_image.shape[0]))
    
    # Stack images horizontally for side-by-side display
    combined_image = cv2.hconcat([original_image, preprocessed_image])
    
    height, width, _ = combined_image.shape
    canvas = tk.Canvas(window, width=width, height=height)
    canvas.pack()
    
    photo = cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB)
    image_tk = ImageTk.PhotoImage(image=Image.fromarray(photo))
    canvas.create_image(width/2, height/2, image=image_tk)
    
    if annotations:
        for annotation in annotations['responses'][0]['textAnnotations'][1:]:
            vertices = annotation['boundingPoly']['vertices']
            pts = [(vertex.get('x', 0), vertex.get('y', 0)) for vertex in vertices]
            
            # Draw on original image
            for i in range(4):
                canvas.create_line(pts[i][0], pts[i][1], 
                                   pts[(i+1)%4][0], pts[(i+1)%4][1], 
                                   fill='green', width=3)
                
            # Draw on preprocessed image with an offset
            for i in range(4):
                canvas.create_line(pts[i][0] + original_image.shape[1], pts[i][1], 
                                   pts[(i+1)%4][0] + original_image.shape[1], pts[(i+1)%4][1], 
                                   fill='green', width=3)
    
    window.mainloop()

def on_proceed_click():
    global preprocess_scale
    global captured_frame  # Ensure you have access to the global variable
    
    if captured_frame is not None:  # Check if captured_frame has been assigned
        print("Processing captured frame for OCR...")
        preprocessed_frame = preprocess_image(captured_frame, preprocess_scale.get())  # Using the selected level
        extracted_text, annotations = send_to_vision(preprocessed_frame)
        
        if extracted_text:
            main_text = annotations['responses'][0]['textAnnotations'][0]['description']
            print(f"Extracted text: {main_text}")
            
            send_to_google_sheets(extracted_text)
            if "DOT" in extracted_text:
                print("DOT found in the extracted text!")
            
            display_image(captured_frame, preprocessed_frame, annotations)
        else:
            print("No text extracted.")
    else:
        print("No captured frame found.")

# Main GUI window
root = tk.Tk()
root.title("Camera OCR Application")

# Add elements to the main GUI
camera_duration_label = tk.Label(root, text="Camera Duration (seconds)")
camera_duration_label.pack()
camera_duration = Scale(root, from_=1, to=5, orient=tk.HORIZONTAL)
camera_duration.pack()

brightness_scale_label = tk.Label(root, text="Brightness factor (1.0 to 3.0)")
brightness_scale_label.pack()
brightness_scale = Scale(root, from_=1, to=3, resolution=0.1, orient=tk.HORIZONTAL)
brightness_scale.set(1.0)
brightness_scale.pack()

# Preprocessing Level
preprocess_scale_label = tk.Label(root, text="Preprocessing Level (1 to 4)")
preprocess_scale_label.pack()
preprocess_scale = Scale(root, from_=1, to=4, orient=tk.HORIZONTAL)
preprocess_scale.pack()

capture_button = tk.Button(root, text="Capture Image", command=lambda: capture_image_from_camera(brightness_scale.get()))
capture_button.pack()

proceed_button = tk.Button(root, text="Process Captured Image", command=on_proceed_click)
proceed_button.pack()

root.mainloop()
